{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          note_id                                              input  \\\n",
      "0  10000032-DS-21  <SEX> F <SERVICE> MEDICINE <ALLERGIES> No Know...   \n",
      "1  10000032-DS-22  <SEX> F <SERVICE> MEDICINE <ALLERGIES> Percoce...   \n",
      "2  10000117-DS-21  <SEX> F <SERVICE> MEDICINE <ALLERGIES> omepraz...   \n",
      "3  10000117-DS-22  <SEX> F <SERVICE> ORTHOPAEDICS <ALLERGIES> ome...   \n",
      "4  10000560-DS-15  <SEX> F <SERVICE> UROLOGY <ALLERGIES> Patient ...   \n",
      "\n",
      "                                              target  input_tokens  \\\n",
      "0  ___ HCV cirrhosis c/b ascites, hiv on ART, h/o...          1946   \n",
      "1  ___ with HIV on HAART, HCV cirrhosis with asci...          2183   \n",
      "2  Ms. ___ is a ___ with history of GERD who pres...          1060   \n",
      "3  The patient presented to the emergency departm...          1195   \n",
      "4  Patient was admitted to Urology after undergoi...           645   \n",
      "\n",
      "   target_tokens  \n",
      "0            231  \n",
      "1            810  \n",
      "2            172  \n",
      "3            330  \n",
      "4            240  \n",
      "(232491, 5)\n"
     ]
    }
   ],
   "source": [
    "bhc = pd.read_csv('D:/Thesis/Dataset/mimic-iv-ehr/mimic-iv-bhc.csv')\n",
    "print(bhc.head())\n",
    "print(bhc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          note_id  subject_id   hadm_id note_type  note_seq  \\\n",
      "0  10000032-DS-21    10000032  22595853        DS        21   \n",
      "1  10000032-DS-22    10000032  22841357        DS        22   \n",
      "2  10000032-DS-23    10000032  29079034        DS        23   \n",
      "3  10000032-DS-24    10000032  25742920        DS        24   \n",
      "4  10000117-DS-21    10000117  22927623        DS        21   \n",
      "\n",
      "             charttime            storetime  \\\n",
      "0  2180-05-07 00:00:00  2180-05-09 15:26:00   \n",
      "1  2180-06-27 00:00:00  2180-07-01 10:15:00   \n",
      "2  2180-07-25 00:00:00  2180-07-25 21:42:00   \n",
      "3  2180-08-07 00:00:00  2180-08-10 05:43:00   \n",
      "4  2181-11-15 00:00:00  2181-11-15 15:04:00   \n",
      "\n",
      "                                                text  \n",
      "0   \\nName:  ___                     Unit No:   _...  \n",
      "1   \\nName:  ___                     Unit No:   _...  \n",
      "2   \\nName:  ___                     Unit No:   _...  \n",
      "3   \\nName:  ___                     Unit No:   _...  \n",
      "4   \\nName:  ___                 Unit No:   ___\\n...  \n",
      "(288112, 8)\n"
     ]
    }
   ],
   "source": [
    "discharge = pd.read_csv('D:/Thesis/Dataset/mimic-iv-ehr/discharge.csv')\n",
    "print(discharge.head())\n",
    "print(discharge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject_id gender  anchor_age  anchor_year anchor_year_group         dod\n",
      "0    10000032      F          52         2180       2014 - 2016  2180-09-09\n",
      "1    10000117      F          48         2174       2008 - 2010         NaN\n",
      "2    10000285      M          34         2159       2017 - 2019         NaN\n",
      "3    10000560      F          53         2189       2008 - 2010         NaN\n",
      "4    10000635      F          74         2136       2014 - 2016         NaN\n"
     ]
    }
   ],
   "source": [
    "patients = pd.read_csv('D:/Thesis/Dataset/mimic-iv-ehr/patients.csv')\n",
    "print(patients.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes:\n",
      "Dataset 1: (288112, 8)\n",
      "Dataset 2: (159608, 6)\n",
      "\n",
      "Merged shape: (288112, 13)\n",
      "\n",
      "First few rows of merged data:\n",
      "          note_id  subject_id   hadm_id note_type  note_seq  \\\n",
      "0  10000032-DS-21    10000032  22595853        DS        21   \n",
      "1  10000032-DS-22    10000032  22841357        DS        22   \n",
      "2  10000032-DS-23    10000032  29079034        DS        23   \n",
      "3  10000032-DS-24    10000032  25742920        DS        24   \n",
      "4  10000117-DS-21    10000117  22927623        DS        21   \n",
      "\n",
      "             charttime            storetime  \\\n",
      "0  2180-05-07 00:00:00  2180-05-09 15:26:00   \n",
      "1  2180-06-27 00:00:00  2180-07-01 10:15:00   \n",
      "2  2180-07-25 00:00:00  2180-07-25 21:42:00   \n",
      "3  2180-08-07 00:00:00  2180-08-10 05:43:00   \n",
      "4  2181-11-15 00:00:00  2181-11-15 15:04:00   \n",
      "\n",
      "                                                text gender  anchor_age  \\\n",
      "0   \\nName:  ___                     Unit No:   _...      F          52   \n",
      "1   \\nName:  ___                     Unit No:   _...      F          52   \n",
      "2   \\nName:  ___                     Unit No:   _...      F          52   \n",
      "3   \\nName:  ___                     Unit No:   _...      F          52   \n",
      "4   \\nName:  ___                 Unit No:   ___\\n...      F          48   \n",
      "\n",
      "   anchor_year anchor_year_group         dod  \n",
      "0         2180       2014 - 2016  2180-09-09  \n",
      "1         2180       2014 - 2016  2180-09-09  \n",
      "2         2180       2014 - 2016  2180-09-09  \n",
      "3         2180       2014 - 2016  2180-09-09  \n",
      "4         2174       2008 - 2010         NaN  \n"
     ]
    }
   ],
   "source": [
    "# Merge datasets\n",
    "merged_df = pd.merge(discharge, patients, on='subject_id', how='inner')\n",
    "\n",
    "# Display info about merged dataset\n",
    "print(\"Original shapes:\")\n",
    "print(f\"Dataset 1: {discharge.shape}\")\n",
    "print(f\"Dataset 2: {patients.shape}\")\n",
    "print(f\"\\nMerged shape: {merged_df.shape}\")\n",
    "\n",
    "# Save merged dataset\n",
    "merged_df.to_csv(\"D:/Thesis/EHR Work/merged_data.csv\", index=False)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst few rows of merged data:\")\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000032\n",
      " \n",
      "Name:  ___                     Unit No:   ___\n",
      " \n",
      "Admission Date:  ___              Discharge Date:   ___\n",
      " \n",
      "Date of Birth:  ___             Sex:   F\n",
      " \n",
      "Service: MEDICINE\n",
      " \n",
      "Allergies: \n",
      "Percocet / Vicodin\n",
      " \n",
      "Attending: ___.\n",
      " \n",
      "Chief Complaint:\n",
      "Abdominal pain\n",
      " \n",
      "Major Surgical or Invasive Procedure:\n",
      "___ Paracentesis\n",
      "\n",
      " \n",
      "History of Present Illness:\n",
      "___ w/ HIV on HAART, COPD on 3L home O2 (though sat'ing fine on \n",
      "RA on admission), HCV cirrhosis c/b ascites requiring biweekly \n",
      "therapeutic paracenteses, hepatic encephalopathy; not on \n",
      "transplant list ___ comorbidities) p/w worsening girth and abd \n",
      "pain. She'd been having pain from her ascites, and felt overdue \n",
      "for a paracentesis. She last had paracentesis on the ___. \n",
      "She reported that she began feeling worsening abdominal pain \n",
      "(constant, epigastric and radiates to back, not a/w food) on \n",
      "___, which increased in severity throughout the course of the \n",
      "day. She was brought to the ED by her son because of this pain. \n",
      "She had no confusion and was alert and oriented x3. The patient \n",
      "also reported she recently ran out of most of her home \n",
      "medications. She denied any fever, chills, shortness of breath, \n",
      "cough, dysuria. She reported loose stool a bit more than usual, \n",
      "but attributed it to lactulose use. In the ED, vitals were: 99.4 \n",
      "105 106/57 18 96% ra. Labs were significant for Na 127 K 5.3 \n",
      "lactate 2.1 INR 1.7. ALT 135 AST 244 AP 123. no leukocytosis. \n",
      "Ascitic fluid showed 220 WBC. She was given Morphine Sulfate 5 \n",
      "mg IV ONCE MR1, and a GI cocktail.\n",
      " \n",
      "Past Medical History:\n",
      "- HCV Cirrhosis: genotype 3a  \n",
      "- HIV: on HAART, ___ CD4 count 173, ___ HIV viral load \n",
      "undetectable  \n",
      "- COPD: ___ PFT showed FVC 1.95 (65%), FEV1 0.88 (37%), \n",
      "FEFmax 2.00 (33%)  \n",
      "- Bipolar Affective Disorder  \n",
      "- PTSD  \n",
      "- Hx of cocaine and heroin abuse  \n",
      "- Hx of skin cancer per patient report  \n",
      " \n",
      "Social History:\n",
      "___\n",
      "Family History:\n",
      "She a total of five siblings, but she is not  talking to most of \n",
      "them. She only has one brother that she is in touch with and \n",
      "lives in ___. She is not aware of any known GI or liver \n",
      "disease in her family.  \n",
      " \n",
      "Physical Exam:\n",
      "ADMISSION PHYSICAL EXAM\n",
      "=======================\n",
      "Vitals - T 97, BP 98/65, HR 103, RR 18, O2 94RA, Glucose 128.  \n",
      "GENERAL: NAD, lying on her right side.\n",
      "HEENT: AT/NC, EOMI, PERRL, anicteric sclera, pink conjunctiva, \n",
      "MMM, good dentition  \n",
      "NECK: nontender supple neck, no LAD, no JVD  \n",
      "CARDIAC: RRR, S1/S2, no murmurs, gallops, or rubs  \n",
      "LUNG: CTAB, no wheezes, rales, rhonchi, breathing comfortably \n",
      "without use of accessory muscles  \n",
      "ABDOMEN: distended, + fluid wave, not tense. NABS, nontender  \n",
      "EXTREMITIES: no cyanosis, clubbing or edema, moving all 4 \n",
      "extremities with purpose\n",
      "PULSES: 2+ DP pulses bilaterally  \n",
      "NEURO: CN II-XII intact, AAO3, no asterxis  \n",
      "SKIN: warm and well perfused, no excoriations or lesions, no \n",
      "rashes\n",
      "\n",
      "DISCHARGE PHYSICAL EXAM\n",
      "=======================\n",
      "Vitals - Tm 98.6, Tc 98.2, BP 82-98/42-68, HR 80-95, RR ___, \n",
      "O2 91-99% RA.\n",
      "GENERAL: NAD, lying on her right side.\n",
      "HEENT: AT/NC, EOMI, PERRL, anicteric sclera, pink conjunctiva, \n",
      "MMM, good dentition  \n",
      "NECK: nontender supple neck, no LAD, no JVD  \n",
      "CARDIAC: RRR, S1/S2, no murmurs, gallops, or rubs  \n",
      "LUNG: CTAB, no wheezes, rales, rhonchi, breathing comfortably \n",
      "without use of accessory muscles  \n",
      "ABDOMEN: soft, decreased distension, not tense. NABS, nontender  \n",
      "\n",
      "EXTREMITIES: no cyanosis, clubbing or edema, moving all 4 \n",
      "extremities with purpose\n",
      "PULSES: 2+ DP pulses bilaterally  \n",
      "NEURO: CN II-XII intact, AAO3, no asterxis  \n",
      "SKIN: warm and well perfused, no excoriations or lesions, no \n",
      "rashes\n",
      " \n",
      "Pertinent Results:\n",
      "ADMISSION LABS\n",
      "==============\n",
      "___ 09:20PM BLOOD WBC-8.2# RBC-3.48* Hgb-12.3 Hct-36.7 \n",
      "MCV-106* MCH-35.5* MCHC-33.6 RDW-16.5* Plt ___\n",
      "___ 09:28PM BLOOD ___ PTT-31.8 ___\n",
      "___ 09:20PM BLOOD Glucose-118* UreaN-33* Creat-0.6 Na-127* \n",
      "K-5.3* Cl-97 HCO3-24 AnGap-11\n",
      "___ 09:20PM BLOOD ALT-135* AST-244* AlkPhos-123* \n",
      "TotBili-1.3\n",
      "___ 09:20PM BLOOD Albumin-3.5 Calcium-8.3* Phos-3.3 Mg-2.3\n",
      "___ 09:27PM BLOOD Lactate-2.1*\n",
      "\n",
      "NOTABLE LABS\n",
      "============\n",
      "___ 12:30AM ASCITES WBC-220* ___ Polys-17* \n",
      "Lymphs-48* Monos-19* Mesothe-2* Macroph-12* Other-2*\n",
      "___ 12:30AM ASCITES TotPro-0.7 Glucose-129\n",
      "\n",
      "MICROBIOLOGY\n",
      "============\n",
      "___ 12:30 am PERITONEAL FLUID\n",
      "\n",
      "   GRAM STAIN (Final ___: \n",
      "      NO POLYMORPHONUCLEAR LEUKOCYTES SEEN. \n",
      "      NO MICROORGANISMS SEEN. \n",
      "\n",
      "   FLUID CULTURE (Final ___:    NO GROWTH. \n",
      "\n",
      "   ANAEROBIC CULTURE (Preliminary):    NO GROWTH.\n",
      "\n",
      "___ VRE screen: No VRE isolated.\n",
      "\n",
      "IMAGING/STUDIES\n",
      "===============\n",
      "CXR ___\n",
      "No acute cardiopulmonary process.\n",
      "\n",
      "RUQUS ___\n",
      "Cirrhotic, mild splenomegaly and ascites.  A solid 3cm nodule in \n",
      "the left lobe of the liver is stable in size and appearance. \n",
      "\n",
      "DISCHARGE LABS\n",
      "==============\n",
      "___ 06:15AM BLOOD WBC-5.6 RBC-3.27* Hgb-11.6* Hct-33.5* \n",
      "MCV-103* MCH-35.5* MCHC-34.5 RDW-16.1* Plt ___\n",
      "___ 06:15AM BLOOD ___ PTT-36.0 ___\n",
      "___ 06:15AM BLOOD Glucose-87 UreaN-29* Creat-0.4 Na-123* \n",
      "K-5.4* Cl-93* HCO3-24 AnGap-11\n",
      "___ 06:15AM BLOOD ALT-112* AST-176* AlkPhos-101 \n",
      "TotBili-2.8*\n",
      "___ 06:15AM BLOOD Albumin-3.5 Calcium-8.8 Phos-3.4 Mg-2.___ w/ HIV on HAART, COPD on 3L home O2, HCV cirrhosis \n",
      "decompensated (ascites requiring biweekly therapeutic \n",
      "paracenteses, hepatic encephalopathy; not on transplant list ___ \n",
      " \n",
      "comorbidities) p/w worsening abd girth and pain.\n",
      "\n",
      "# Goals of care:  The patient expressed that her desire to \"let \n",
      "go\" and be at peace. She is tired of \"fighting\" her end stage \n",
      "liver disease and does not feel that she has a good quality of \n",
      "life. She wished to meet with a representative from hospice, \n",
      "which we were able to arrange, and she decided that she would \n",
      "like to go home with hospice. A conversation was held with the \n",
      "patient regarding code status. She noted that she would probably \n",
      "be DNR/DNI, but would like to discuss it with her son (HCP). \n",
      "Therefore, at this time, she remains full code. We also \n",
      "discussed the possibility of her getting a pleurex catheter \n",
      "placed because of her repeated paracenteses. She was discharged \n",
      "to home with hospice on ___.\n",
      "\n",
      "# Decompensated HCV cirrhosis with ascites: The patient \n",
      "presented with worsening ascites. Her pain was initially managed \n",
      "with tramadol and morphine. She underwent ___ paracentesis \n",
      "on ___ with removal of 3.5L and she received albumin. Studies \n",
      "on the fluid were negative, and she reported a significant \n",
      "improvement in her pain.\n",
      " \n",
      "# Hyperkalemia: During her hospitalization, it was noted that \n",
      "she had hyperkalemia 5.3 -> 5.9 -> 6.9 without EKG changes. \n",
      "After treatment with calcium gluconate 2gm x2, insulin and D50 x \n",
      "2, and kayexelate, her K downtrended to 5.4. Her K has been \n",
      "persistently elevated in the past and should continue to be \n",
      "monitored.\n",
      "\n",
      "# Hepatic encephalopathy: The patient's mental status was A+Ox3 \n",
      "on admission. She was continued on rifaximin and lactulose \n",
      "in-house and at discharge.\n",
      "\n",
      "CHRONIC ISSUES\n",
      "==============\n",
      "\n",
      "# Hyponatremia - The patient has a history of asymptomatic \n",
      "hyponatremia. It is most likely hypervolemic hyponatremia \n",
      "related to underlying liver disease as well as aldosterone axis \n",
      "dysfunction related to liver disease. She remained asymptomatic \n",
      "through her hospitalization. Because she was also started on \n",
      "10mg of furosemide to help prevent fluid re-accumulation, she \n",
      "will require BMPs twice weekly in the outpatient setting.\n",
      "\n",
      "# HIV: Emtricitabine-Tenofovir (Truvada) and Raltegravir were \n",
      "continued in-house and at discharge.  \n",
      "\n",
      "# COPD: fluticasone, tiotropium and albuterol were continued \n",
      "in-house and at discharge.\n",
      "\n",
      "TRANSITIONAL ISSUES\n",
      "===================\n",
      "[ ] Follow-up peritoneal fluid culture\n",
      "[ ] New/changed medications: Furosemide 10mg daily (new)\n",
      "[ ] For hyperkalemia and hyponatremia, the patient should have \n",
      "biweekly BMPs (script written). If K > 5.3, she should be given \n",
      "kayexelate once weekly. Lab results will be faxed/sent to Dr. \n",
      "___.\n",
      "[ ] Continue conversation regarding pleurex catheter placement \n",
      "and code status.\n",
      "[ ] She will continue to have twice weekly paracenteses\n",
      "\n",
      "# Code: full  \n",
      "# Emergency Contact: ___ ___ \n",
      " \n",
      "___ on Admission:\n",
      "The Preadmission Medication list is accurate and complete.\n",
      "1. Calcium Carbonate 500 mg PO BID \n",
      "2. Emtricitabine-Tenofovir (Truvada) 1 TAB PO DAILY \n",
      "3. Fluticasone Propionate 110mcg 1 PUFF IH BID \n",
      "4. Lactulose 30 mL PO TID \n",
      "5. Raltegravir 400 mg PO BID \n",
      "6. Rifaximin 550 mg PO BID \n",
      "7. TraMADOL (Ultram) ___ mg PO Q6H:PRN pain \n",
      "8. albuterol sulfate 90 mcg/actuation inhalation Q6H:PRN \n",
      "Wheezing \n",
      "9. Tiotropium Bromide 1 CAP IH DAILY \n",
      "\n",
      " \n",
      "Discharge Medications:\n",
      "1. albuterol sulfate 90 mcg/actuation inhalation Q6H:PRN \n",
      "Wheezing \n",
      "2. Calcium Carbonate 500 mg PO BID \n",
      "3. Emtricitabine-Tenofovir (Truvada) 1 TAB PO DAILY \n",
      "4. Fluticasone Propionate 110mcg 1 PUFF IH BID \n",
      "5. Lactulose 30 mL PO TID \n",
      "6. Raltegravir 400 mg PO BID \n",
      "7. Rifaximin 550 mg PO BID \n",
      "8. Tiotropium Bromide 1 CAP IH DAILY \n",
      "9. TraMADOL (Ultram) ___ mg PO Q6H:PRN pain \n",
      "10. Furosemide 10 mg PO DAILY \n",
      "RX *furosemide 20 mg 0.5 (One half) tablet(s) by mouth Please \n",
      "take 1 tablet daily. Disp #*15 Tablet Refills:*0\n",
      "11. Outpatient Lab Work\n",
      "Please obtain biweekly BMP to monitor K and Na. Please fax \n",
      "results to Dr. ___ at the ___ at ___.\n",
      "\n",
      " \n",
      "Discharge Disposition:\n",
      "Home With Service\n",
      " \n",
      "Facility:\n",
      "___\n",
      " \n",
      "Discharge Diagnosis:\n",
      "PRIMARY DIAGNOSIS\n",
      "=================\n",
      "-Decompensated HCV cirrhosis\n",
      "-Hyperkalemia\n",
      "\n",
      "SECONDARY DIAGNOSES\n",
      "===================\n",
      "-HIV\n",
      "-COPD\n",
      "-Hyponatremia\n",
      "\n",
      " \n",
      "Discharge Condition:\n",
      "Mental Status: Clear and coherent.\n",
      "Level of Consciousness: Alert and interactive.\n",
      "Activity Status: Ambulatory - Independent.\n",
      "\n",
      " \n",
      "Discharge Instructions:\n",
      "Dear Ms. ___,\n",
      "\n",
      "It was a pleasure caring for you during your stay at ___. You \n",
      "were admitted because of worsening abdominal pain. It is likely \n",
      "that this pain was due to increasing fluid in the abdomen from \n",
      "your cirrhosis. After a paracentesis in which much of this fluid \n",
      "was removed, your pain improved significantly. You were noted to \n",
      "have a high potassium during your hospitalization, which came \n",
      "down with treatment but this should continue to be followed as \n",
      "an outpatient. You also emphasized your desire to speak with a \n",
      "representative from hospice. Because of your repeated \n",
      "paracenteses, we also discussed the possibility of a pleurex \n",
      "catheter placement. We arranged for you to meet with hospice \n",
      "coordinators, and you will be discharged home with hospice.\n",
      "\n",
      "We wish you all the best!\n",
      "\n",
      "Your ___ care team\n",
      " \n",
      "Followup Instructions:\n",
      "___\n",
      "\n"
     ]
    }
   ],
   "source": [
    "entry1 = discharge.loc[3, 'subject_id']\n",
    "entry2 = discharge.loc[3, 'text']\n",
    "print(entry1)\n",
    "print(entry2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            note_id  subject_id   hadm_id note_type  note_seq  \\\n",
      "15   10000980-DS-20    10000980  29654838        DS        20   \n",
      "17   10000980-DS-22    10000980  24947999        DS        22   \n",
      "22   10001176-DS-17    10001176  23334588        DS        17   \n",
      "42   10001884-DS-24    10001884  21268656        DS        24   \n",
      "48   10001884-DS-30    10001884  26170293        DS        30   \n",
      "52   10001884-DS-34    10001884  28664981        DS        34   \n",
      "59   10002013-DS-11    10002013  27574273        DS        11   \n",
      "67    10002013-DS-9    10002013  28420602        DS         9   \n",
      "139   10003502-DS-3    10003502  21671572        DS         3   \n",
      "153  10004235-DS-21    10004235  24181354        DS        21   \n",
      "157  10004322-DS-22    10004322  22602599        DS        22   \n",
      "160  10004401-DS-23    10004401  25753439        DS        23   \n",
      "169  10004401-DS-32    10004401  26488315        DS        32   \n",
      "171  10004422-DS-19    10004422  21255400        DS        19   \n",
      "172  10004457-DS-10    10004457  28723315        DS        10   \n",
      "210  10005858-DS-16    10005858  29569314        DS        16   \n",
      "237   10006692-DS-5    10006692  29746536        DS         5   \n",
      "264  10009049-DS-16    10009049  22995465        DS        16   \n",
      "267   10009203-DS-9    10009203  23598550        DS         9   \n",
      "268   10009585-DS-9    10009585  23010368        DS         9   \n",
      "280  10010058-DS-13    10010058  21955805        DS        13   \n",
      "281  10010058-DS-14    10010058  26359957        DS        14   \n",
      "283  10010231-DS-20    10010231  25499227        DS        20   \n",
      "289  10010231-DS-26    10010231  20687038        DS        26   \n",
      "294   10010393-DS-4    10010393  28846987        DS         4   \n",
      "\n",
      "               charttime            storetime  \\\n",
      "15   2188-01-05 00:00:00  2188-01-06 20:49:00   \n",
      "17   2190-11-08 00:00:00  2190-11-09 13:57:00   \n",
      "22   2186-12-02 00:00:00  2186-12-02 16:38:00   \n",
      "42   2125-10-20 00:00:00  2125-10-24 14:10:00   \n",
      "48   2130-04-19 00:00:00  2130-04-22 13:31:00   \n",
      "52   2130-11-30 00:00:00  2130-12-01 21:56:00   \n",
      "59   2164-03-19 00:00:00  2164-03-29 20:24:00   \n",
      "67   2161-02-09 00:00:00  2161-02-12 05:52:00   \n",
      "139  2163-02-06 00:00:00  2163-02-07 15:26:00   \n",
      "153  2196-03-04 00:00:00  2196-03-06 10:57:00   \n",
      "157  2134-10-02 00:00:00  2134-10-09 14:09:00   \n",
      "160  2142-08-29 00:00:00  2142-08-29 19:03:00   \n",
      "169  2144-05-22 00:00:00  2144-05-23 10:13:00   \n",
      "171  2111-01-25 00:00:00  2111-01-25 11:20:00   \n",
      "172  2141-08-13 00:00:00  2141-08-13 18:50:00   \n",
      "210  2177-07-21 00:00:00  2177-07-23 12:30:00   \n",
      "237  2165-05-13 00:00:00  2165-05-14 07:35:00   \n",
      "264  2174-05-31 00:00:00  2174-06-03 15:00:00   \n",
      "267  2201-08-14 00:00:00  2201-08-16 13:02:00   \n",
      "268  2111-04-17 00:00:00  2111-04-17 14:14:00   \n",
      "280  2147-01-06 00:00:00  2147-01-07 18:42:00   \n",
      "281  2147-11-19 00:00:00  2147-11-19 05:47:00   \n",
      "283  2117-12-05 00:00:00  2117-12-06 05:13:00   \n",
      "289  2118-03-09 00:00:00  2118-03-09 15:57:00   \n",
      "294  2134-12-12 00:00:00  2134-12-12 19:20:00   \n",
      "\n",
      "                                                  text gender  anchor_age  \\\n",
      "15    \\nName:  ___          Unit No:   ___\\n \\nAdmi...      F          73   \n",
      "17    \\nName:  ___          Unit No:   ___\\n \\nAdmi...      F          73   \n",
      "22    \\nName:  ___                   Unit No:   ___...      F          64   \n",
      "42    \\nName:  ___             Unit No:   ___\\n \\nA...      F          68   \n",
      "48    \\nName:  ___             Unit No:   ___\\n \\nA...      F          68   \n",
      "52    \\nName:  ___             Unit No:   ___\\n \\nA...      F          68   \n",
      "59    \\nName:  ___                 Unit No:   ___\\n...      F          53   \n",
      "67    \\nName:  ___                 Unit No:   ___\\n...      F          53   \n",
      "139   \\nName:  ___                 Unit No:   ___\\n...      F          86   \n",
      "153   \\nName:  ___                  Unit No:   ___\\...      M          47   \n",
      "157   \\nName:  ___                 Unit No:   ___\\n...      M          60   \n",
      "160   \\nName:  ___              Unit No:   ___\\n \\n...      M          82   \n",
      "169   \\nName:  ___              Unit No:   ___\\n \\n...      M          82   \n",
      "171   \\nName:  ___                 Unit No:   ___\\n...      M          78   \n",
      "172   \\nName:  ___.             Unit No:   ___\\n \\n...      M          65   \n",
      "210   \\nName:  ___.                 Unit No:   ___\\...      F          62   \n",
      "237   \\nName:  ___                    Unit No:   __...      M          69   \n",
      "264   \\nName:  ___                    Unit No:   __...      M          56   \n",
      "267   \\nName:  ___               Unit No:   ___\\n \\...      M          57   \n",
      "268   \\nName:  ___                   Unit No:   ___...      M          91   \n",
      "280   \\nName:  ___                      Unit No:   ...      M          80   \n",
      "281   \\nName:  ___                      Unit No:   ...      M          80   \n",
      "283   \\nName:  ___                 Unit No:   ___\\n...      M          57   \n",
      "289   \\nName:  ___                 Unit No:   ___\\n...      M          57   \n",
      "294   \\nName:  ___                  Unit No:   ___\\...      F          46   \n",
      "\n",
      "     anchor_year anchor_year_group         dod  \n",
      "15          2186       2008 - 2010  2193-08-26  \n",
      "17          2186       2008 - 2010  2193-08-26  \n",
      "22          2186       2011 - 2013         NaN  \n",
      "42          2122       2008 - 2010  2131-01-20  \n",
      "48          2122       2008 - 2010  2131-01-20  \n",
      "52          2122       2008 - 2010  2131-01-20  \n",
      "59          2156       2008 - 2010         NaN  \n",
      "67          2156       2008 - 2010         NaN  \n",
      "139         2161       2008 - 2010  2169-09-10  \n",
      "153         2196       2014 - 2016         NaN  \n",
      "157         2131       2011 - 2013         NaN  \n",
      "160         2141       2008 - 2010  2144-06-18  \n",
      "169         2141       2008 - 2010  2144-06-18  \n",
      "171         2111       2011 - 2013         NaN  \n",
      "172         2140       2011 - 2013         NaN  \n",
      "210         2167       2008 - 2010         NaN  \n",
      "237         2165       2014 - 2016         NaN  \n",
      "264         2174       2014 - 2016         NaN  \n",
      "267         2201       2011 - 2013         NaN  \n",
      "268         2111       2011 - 2013  2111-08-29  \n",
      "280         2139       2011 - 2013  2147-11-19  \n",
      "281         2139       2011 - 2013  2147-11-19  \n",
      "283         2117       2014 - 2016         NaN  \n",
      "289         2117       2014 - 2016         NaN  \n",
      "294         2134       2014 - 2016         NaN  \n"
     ]
    }
   ],
   "source": [
    "ecg_mentions = merged_df[merged_df['text'].str.contains('ecg', case=False, na=False)]\n",
    "\n",
    "print(ecg_mentions.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           note_id  subject_id   hadm_id note_type  note_seq  \\\n",
      "2   10000032-DS-23    10000032  29079034        DS        23   \n",
      "3   10000032-DS-24    10000032  25742920        DS        24   \n",
      "12  10000935-DS-19    10000935  21738619        DS        19   \n",
      "14  10000935-DS-21    10000935  25849114        DS        21   \n",
      "16  10000980-DS-21    10000980  26913865        DS        21   \n",
      "17  10000980-DS-22    10000980  24947999        DS        22   \n",
      "20  10000980-DS-25    10000980  29659838        DS        25   \n",
      "21  10000980-DS-26    10000980  20897796        DS        26   \n",
      "28   10001338-DS-6    10001338  22119639        DS         6   \n",
      "33  10001401-DS-19    10001401  24818636        DS        19   \n",
      "35  10001401-DS-21    10001401  28058085        DS        21   \n",
      "42  10001884-DS-24    10001884  21268656        DS        24   \n",
      "43  10001884-DS-25    10001884  26679629        DS        25   \n",
      "45  10001884-DS-27    10001884  21577720        DS        27   \n",
      "46  10001884-DS-28    10001884  25758848        DS        28   \n",
      "47  10001884-DS-29    10001884  29675586        DS        29   \n",
      "48  10001884-DS-30    10001884  26170293        DS        30   \n",
      "51  10001884-DS-33    10001884  28475784        DS        33   \n",
      "53  10001884-DS-35    10001884  24962904        DS        35   \n",
      "54  10001884-DS-36    10001884  27507515        DS        36   \n",
      "59  10002013-DS-11    10002013  27574273        DS        11   \n",
      "60  10002013-DS-12    10002013  21763296        DS        12   \n",
      "63   10002013-DS-5    10002013  25715803        DS         5   \n",
      "64   10002013-DS-6    10002013  21975601        DS         6   \n",
      "66   10002013-DS-8    10002013  24760295        DS         8   \n",
      "\n",
      "              charttime            storetime  \\\n",
      "2   2180-07-25 00:00:00  2180-07-25 21:42:00   \n",
      "3   2180-08-07 00:00:00  2180-08-10 05:43:00   \n",
      "12  2187-07-12 00:00:00  2187-07-12 14:01:00   \n",
      "14  2187-10-26 00:00:00  2187-10-27 15:36:00   \n",
      "16  2189-07-03 00:00:00  2189-07-03 19:50:00   \n",
      "17  2190-11-08 00:00:00  2190-11-09 13:57:00   \n",
      "20  2191-07-19 00:00:00  2191-07-22 09:37:00   \n",
      "21  2193-08-17 00:00:00  2193-08-17 16:05:00   \n",
      "28  2138-05-27 00:00:00  2138-05-27 14:21:00   \n",
      "33  2131-08-04 00:00:00  2131-08-04 15:04:00   \n",
      "35  2131-11-15 00:00:00  2131-11-20 19:51:00   \n",
      "42  2125-10-20 00:00:00  2125-10-24 14:10:00   \n",
      "43  2125-10-27 00:00:00  2125-10-27 18:03:00   \n",
      "45  2125-12-27 00:00:00  2125-12-29 20:03:00   \n",
      "46  2128-07-17 00:00:00  2128-07-17 13:55:00   \n",
      "47  2130-04-09 00:00:00  2130-04-13 17:53:00   \n",
      "48  2130-04-19 00:00:00  2130-04-22 13:31:00   \n",
      "51  2130-10-22 00:00:00  2130-10-22 21:49:00   \n",
      "53  2130-12-08 00:00:00  2130-12-13 21:50:00   \n",
      "54  2130-12-24 00:00:00  2130-12-26 17:14:00   \n",
      "59  2164-03-19 00:00:00  2164-03-29 20:24:00   \n",
      "60  2165-11-26 00:00:00  2165-11-26 18:46:00   \n",
      "63  2156-11-05 00:00:00  2156-12-04 07:04:00   \n",
      "64  2159-12-17 00:00:00  2159-12-21 10:02:00   \n",
      "66  2160-07-12 00:00:00  2160-07-14 13:59:00   \n",
      "\n",
      "                                                 text gender  anchor_age  \\\n",
      "2    \\nName:  ___                     Unit No:   _...      F          52   \n",
      "3    \\nName:  ___                     Unit No:   _...      F          52   \n",
      "12   \\nName:  ___                   Unit No:   ___...      F          52   \n",
      "14   \\nName:  ___                   Unit No:   ___...      F          52   \n",
      "16   \\nName:  ___          Unit No:   ___\\n \\nAdmi...      F          73   \n",
      "17   \\nName:  ___          Unit No:   ___\\n \\nAdmi...      F          73   \n",
      "20   \\nName:  ___          Unit No:   ___\\n \\nAdmi...      F          73   \n",
      "21   \\nName:  ___          Unit No:   ___\\n \\nAdmi...      F          73   \n",
      "28   \\nName:  ___                    Unit No:   __...      F          43   \n",
      "33   \\nName:  ___              Unit No:   ___\\n \\n...      F          89   \n",
      "35   \\nName:  ___              Unit No:   ___\\n \\n...      F          89   \n",
      "42   \\nName:  ___             Unit No:   ___\\n \\nA...      F          68   \n",
      "43   \\nName:  ___             Unit No:   ___\\n \\nA...      F          68   \n",
      "45   \\nName:  ___             Unit No:   ___\\n \\nA...      F          68   \n",
      "46   \\nName:  ___             Unit No:   ___\\n \\nA...      F          68   \n",
      "47   \\nName:  ___             Unit No:   ___\\n \\nA...      F          68   \n",
      "48   \\nName:  ___             Unit No:   ___\\n \\nA...      F          68   \n",
      "51   \\nName:  ___             Unit No:   ___\\n \\nA...      F          68   \n",
      "53   \\nName:  ___             Unit No:   ___\\n \\nA...      F          68   \n",
      "54   \\nName:  ___             Unit No:   ___\\n \\nA...      F          68   \n",
      "59   \\nName:  ___                 Unit No:   ___\\n...      F          53   \n",
      "60   \\nName:  ___                 Unit No:   ___\\n...      F          53   \n",
      "63   \\nName:  ___                 Unit No:   ___\\n...      F          53   \n",
      "64   \\nName:  ___                 Unit No:   ___\\n...      F          53   \n",
      "66   \\nName:  ___                 Unit No:   ___\\n...      F          53   \n",
      "\n",
      "    anchor_year anchor_year_group         dod  \n",
      "2          2180       2014 - 2016  2180-09-09  \n",
      "3          2180       2014 - 2016  2180-09-09  \n",
      "12         2182       2008 - 2010  2187-11-12  \n",
      "14         2182       2008 - 2010  2187-11-12  \n",
      "16         2186       2008 - 2010  2193-08-26  \n",
      "17         2186       2008 - 2010  2193-08-26  \n",
      "20         2186       2008 - 2010  2193-08-26  \n",
      "21         2186       2008 - 2010  2193-08-26  \n",
      "28         2138       2008 - 2010         NaN  \n",
      "33         2131       2014 - 2016         NaN  \n",
      "35         2131       2014 - 2016         NaN  \n",
      "42         2122       2008 - 2010  2131-01-20  \n",
      "43         2122       2008 - 2010  2131-01-20  \n",
      "45         2122       2008 - 2010  2131-01-20  \n",
      "46         2122       2008 - 2010  2131-01-20  \n",
      "47         2122       2008 - 2010  2131-01-20  \n",
      "48         2122       2008 - 2010  2131-01-20  \n",
      "51         2122       2008 - 2010  2131-01-20  \n",
      "53         2122       2008 - 2010  2131-01-20  \n",
      "54         2122       2008 - 2010  2131-01-20  \n",
      "59         2156       2008 - 2010         NaN  \n",
      "60         2156       2008 - 2010         NaN  \n",
      "63         2156       2008 - 2010         NaN  \n",
      "64         2156       2008 - 2010         NaN  \n",
      "66         2156       2008 - 2010         NaN  \n"
     ]
    }
   ],
   "source": [
    "ekg_mentions = merged_df[merged_df['text'].str.contains('ekg', case=False, na=False)]\n",
    "\n",
    "print(ekg_mentions.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EKG was mentioned 77185 times.\n",
      "EKG was mentioned at least once in 77185 rows.\n"
     ]
    }
   ],
   "source": [
    "ekg_count = merged_df['text'].str.contains('ekg', case=False, na=False).sum()\n",
    "# Count only one mention of 'ecg' per row\n",
    "ekg_mention_per_row = merged_df['text'].str.contains('ekg', case=False, na=False).sum()\n",
    "\n",
    "print(f\"EKG was mentioned {ekg_count} times.\")\n",
    "print(f\"EKG was mentioned at least once in {ekg_mention_per_row} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset with partial matches saved to D:/Thesis/EHR Work/filtered_dataset_matches_ECGorEKG.csv\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "merged_data = 'D:/Thesis/EHR Work/merged_data.csv'  # Replace with your actual file path\n",
    "df = pd.read_csv(merged_data)\n",
    "\n",
    "# Assuming the column with text is named 'text_column' - replace with the actual column name\n",
    "filtered_df = df[df['text'].str.contains(r'ECG|EKG', case=False, na=False)]\n",
    "\n",
    "# Save the filtered rows to a new CSV\n",
    "output_path = 'D:/Thesis/EHR Work/filtered_dataset_matches_ECGorEKG.csv'  # Replace with desired output file path\n",
    "filtered_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered dataset with partial matches saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          note_id  subject_id   hadm_id note_type  note_seq  \\\n",
      "0  10000032-DS-23    10000032  29079034        DS        23   \n",
      "1  10000032-DS-24    10000032  25742920        DS        24   \n",
      "2  10000935-DS-19    10000935  21738619        DS        19   \n",
      "3  10000935-DS-21    10000935  25849114        DS        21   \n",
      "4  10000980-DS-20    10000980  29654838        DS        20   \n",
      "\n",
      "             charttime            storetime  \\\n",
      "0  2180-07-25 00:00:00  2180-07-25 21:42:00   \n",
      "1  2180-08-07 00:00:00  2180-08-10 05:43:00   \n",
      "2  2187-07-12 00:00:00  2187-07-12 14:01:00   \n",
      "3  2187-10-26 00:00:00  2187-10-27 15:36:00   \n",
      "4  2188-01-05 00:00:00  2188-01-06 20:49:00   \n",
      "\n",
      "                                                text gender  anchor_age  \\\n",
      "0   \\nName:  ___                     Unit No:   _...      F          52   \n",
      "1   \\nName:  ___                     Unit No:   _...      F          52   \n",
      "2   \\nName:  ___                   Unit No:   ___...      F          52   \n",
      "3   \\nName:  ___                   Unit No:   ___...      F          52   \n",
      "4   \\nName:  ___          Unit No:   ___\\n \\nAdmi...      F          73   \n",
      "\n",
      "   anchor_year anchor_year_group         dod  \n",
      "0         2180       2014 - 2016  2180-09-09  \n",
      "1         2180       2014 - 2016  2180-09-09  \n",
      "2         2182       2008 - 2010  2187-11-12  \n",
      "3         2182       2008 - 2010  2187-11-12  \n",
      "4         2186       2008 - 2010  2193-08-26  \n",
      "(94297, 13)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('D:/Thesis/EHR Work/filtered_dataset_matches_ECGorEKG.csv')\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed data has been saved to D:/Thesis/EHR Work/parsed_ecg_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load your dataset\n",
    "input_file = 'D:/Thesis/EHR Work/filtered_dataset_matches_ECGorEKG.csv'\n",
    "output_file = 'D:/Thesis/EHR Work/parsed_ecg_data.csv'\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Define a function to extract X labels and Y labels from the text column\n",
    "def extract_labels(text):\n",
    "    # Initialize a dictionary to hold the extracted data\n",
    "    extracted_data = {\n",
    "        \"Sex\": None,\n",
    "        \"Chief Complaint\": None,\n",
    "        \"History of Present Illness\": None,\n",
    "        \"Past Medical History\": None,\n",
    "        \"Family History\": None,\n",
    "        \"Physical Exam on Admission\": None,\n",
    "        \"ECG/EKG Sentences\": None,\n",
    "    }\n",
    "\n",
    "    # Define regex patterns for each section\n",
    "    patterns = {\n",
    "        \"Sex\": r\"Sex:\\s*([FM])\",  # Only capture \"F\" or \"M\"\n",
    "        \"Chief Complaint\": r\"Chief Complaint:\\s*(.*?)(?=\\n[A-Z])\",  # Stop at the next uppercase line\n",
    "        \"History of Present Illness\": r\"History of Present Illness:\\s*(.*?)(?=\\n[A-Z])\",  # Stop at the next uppercase line\n",
    "        \"Past Medical History\": r\"Past Medical History:\\s*(.*?)(?=\\n[A-Z])\",  # Stop at the next uppercase line\n",
    "        \"Family History\": r\"Family History:\\s*(.*?)(?=\\n[A-Z])\",  # Stop at the next uppercase line\n",
    "        \"Physical Exam on Admission\": r\"(?:Admission Exam:|On admission|ADMISSION PHYSICAL EXAM:)\\s*(.*?)(?=\\n[A-Z])\",  # Capture content after headers\n",
    "    }\n",
    "\n",
    "    # Extract sections based on patterns\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            extracted_data[key] = match.group(1).strip()\n",
    "\n",
    "    # Extract sentences with \"ECG\" or \"EKG\"\n",
    "    ecg_sentences = re.findall(r'([^\\.]*?(?:ECG|EKG)[^\\.]*\\.)', text, re.IGNORECASE)\n",
    "    \n",
    "    # Filter out sentences that are just headers or noise\n",
    "    filtered_ecg_sentences = []\n",
    "    for sentence in ecg_sentences:\n",
    "        # Exclude sentences that are just headers or irrelevant content\n",
    "        if not re.match(r'^\\s*(?:ECG|EKG)\\s*[:;,-]?\\s*$', sentence, re.IGNORECASE):\n",
    "            filtered_ecg_sentences.append(sentence.strip())\n",
    "    \n",
    "    # Join the filtered sentences\n",
    "    extracted_data[\"ECG/EKG Sentences\"] = \" \".join(filtered_ecg_sentences)\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "# Apply the extraction function to the dataset\n",
    "parsed_data = data['text'].apply(extract_labels)\n",
    "\n",
    "# Convert the parsed data back to a Pandas DataFrame\n",
    "parsed_df = pd.DataFrame(parsed_data.tolist())\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "parsed_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Parsed data has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Sex, Chief Complaint, History of Present Illness, Past Medical History, Family History, Physical Exam on Admission, ECG/EKG Sentences]\n",
      "Index: []\n",
      "(94297, 7)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('D:/Thesis/EHR Work/parsed_ecg_data.csv')\n",
    "print(df.head(0))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\mehed\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"D:/Thesis/EHR Work/parsed_ecg_data.csv\")\n",
    "\n",
    "# Combine input columns into a single text column\n",
    "df['input_text'] = (\n",
    "    \"Sex: \" + df['Sex'].fillna(\"\") + \" | \" +\n",
    "    \"Chief Complaint: \" + df['Chief Complaint'].fillna(\"\") + \" | \" +\n",
    "    \"History of Present Illness: \" + df['History of Present Illness'].fillna(\"\") + \" | \" +\n",
    "    \"Past Medical History: \" + df['Past Medical History'].fillna(\"\") + \" | \" +\n",
    "    \"Family History: \" + df['Family History'].fillna(\"\") + \" | \" +\n",
    "    \"Physical Exam on Admission: \" + df['Physical Exam on Admission'].fillna(\"\")\n",
    ")\n",
    "\n",
    "# Encode the target column (\"ECG/EKG Sentences\")\n",
    "label_encoder = LabelEncoder()\n",
    "df['target'] = label_encoder.fit_transform(df['ECG/EKG Sentences'].astype(str))\n",
    "\n",
    "# Split the dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['input_text'].tolist(), df['target'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# Load the pre-trained BERT model for classification\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Set up training arguments with parallel data loading\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    dataloader_num_workers=4,  # Enable parallel data loading with 4 workers\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./ecg_bert_model\")\n",
    "tokenizer.save_pretrained(\"./ecg_bert_model\")\n",
    "\n",
    "print(\"Model training complete and saved!\")\n",
    "\n",
    "# Predicting on new data\n",
    "def predict_ecg(texts, model, tokenizer):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    return label_encoder.inverse_transform(predictions.numpy())\n",
    "\n",
    "# Example usage\n",
    "sample_texts = df['input_text'].iloc[:5].tolist()\n",
    "predictions = predict_ecg(sample_texts, model, tokenizer)\n",
    "print(\"Sample Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mehed\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\mehed\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28290' max='28290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28290/28290 2:03:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11.699300</td>\n",
       "      <td>11.680066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11.316000</td>\n",
       "      <td>11.646075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.339400</td>\n",
       "      <td>12.548532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete and saved!\n",
      "Sample Predictions: ['EKG: Sinus rhythm.' 'EKG: Sinus rhythm.' 'EKG: Sinus rhythm.'\n",
      " 'EKG: Sinus rhythm.' 'EKG: Sinus rhythm.']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"D:/Thesis/EHR Work/parsed_ecg_data.csv\")\n",
    "\n",
    "# Combine input columns into a single text column\n",
    "df['input_text'] = (\n",
    "    \"Sex: \" + df['Sex'].fillna(\"\") + \" | \" +\n",
    "    \"Chief Complaint: \" + df['Chief Complaint'].fillna(\"\") + \" | \" +\n",
    "    \"History of Present Illness: \" + df['History of Present Illness'].fillna(\"\") + \" | \" +\n",
    "    \"Past Medical History: \" + df['Past Medical History'].fillna(\"\") + \" | \" +\n",
    "    \"Family History: \" + df['Family History'].fillna(\"\") + \" | \" +\n",
    "    \"Physical Exam on Admission: \" + df['Physical Exam on Admission'].fillna(\"\")\n",
    ")\n",
    "\n",
    "# Encode the target column (\"ECG/EKG Sentences\")\n",
    "label_encoder = LabelEncoder()\n",
    "df['target'] = label_encoder.fit_transform(df['ECG/EKG Sentences'].astype(str))\n",
    "\n",
    "# Split the dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['input_text'].tolist(), df['target'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# Load the pre-trained BERT model for classification\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./ecg_bert_model\")\n",
    "tokenizer.save_pretrained(\"./ecg_bert_model\")\n",
    "\n",
    "print(\"Model training complete and saved!\")\n",
    "\n",
    "# Predicting on new data\n",
    "def predict_ecg(texts, model, tokenizer):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    return label_encoder.inverse_transform(predictions.cpu().numpy())\n",
    "\n",
    "# Example usage\n",
    "sample_texts = df['input_text'].iloc[:5].tolist()\n",
    "predictions = predict_ecg(sample_texts, model, tokenizer)\n",
    "print(\"Sample Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.10%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Function to evaluate the model on the validation set\n",
    "def evaluate_model(model, val_dataset, tokenizer, batch_size=8):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Move the model to the appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Create a DataLoader for the validation dataset\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Lists to store predictions and true labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Iterate over the validation DataLoader\n",
    "    for batch in val_dataloader:\n",
    "        # Move input tensors to the same device as the model\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "            \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "        }\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "        # Store predictions and labels\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = evaluate_model(model, val_dataset, tokenizer)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
